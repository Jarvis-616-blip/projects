{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d869ab",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch: Classifying Letters A, B, and C\n",
    "\n",
    "This notebook implements a simple feedforward neural network **from scratch** using only NumPy.\n",
    "The goal is to classify 5×6 binary images of letters **A**, **B**, and **C**.\n",
    "\n",
    "## Outline\n",
    "1. Create binary image patterns for A, B, and C (5×6 grid → 30 pixels)\n",
    "2. Build a small dataset (with some noisy variants)\n",
    "3. Define a two-layer neural network (input → hidden → output)\n",
    "4. Implement forward pass, loss computation, and backpropagation manually\n",
    "5. Train with gradient descent and track loss & accuracy\n",
    "6. Visualize training curves and test predictions (with `imshow`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d39ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8aa1d",
   "metadata": {},
   "source": [
    "## 1. Create Binary Patterns for Letters A, B, and C\n",
    "Each letter is represented as a 5×6 grid of 0s and 1s. We flatten this grid into a 1D array of length 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pattern(pattern, title=\"\"):\n",
    "    \"\"\"Utility to visualize a 5x6 binary pattern.\"\"\"\n",
    "    plt.imshow(pattern.reshape(5, 6), cmap='gray_r')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Manually define base 5x6 patterns for A, B, C\n",
    "# You can tweak these shapes if you like, but keep them 5x6.\n",
    "A_pattern = np.array([\n",
    "    [0,1,1,1,1,0],\n",
    "    [1,0,0,0,0,1],\n",
    "    [1,1,1,1,1,1],\n",
    "    [1,0,0,0,0,1],\n",
    "    [1,0,0,0,0,1]\n",
    "], dtype=np.float32).reshape(-1)\n",
    "\n",
    "B_pattern = np.array([\n",
    "    [1,1,1,1,0,0],\n",
    "    [1,0,0,0,1,0],\n",
    "    [1,1,1,1,0,0],\n",
    "    [1,0,0,0,1,0],\n",
    "    [1,1,1,1,0,0]\n",
    "], dtype=np.float32).reshape(-1)\n",
    "\n",
    "C_pattern = np.array([\n",
    "    [0,1,1,1,1,0],\n",
    "    [1,0,0,0,0,1],\n",
    "    [1,0,0,0,0,0],\n",
    "    [1,0,0,0,0,1],\n",
    "    [0,1,1,1,1,0]\n",
    "], dtype=np.float32).reshape(-1)\n",
    "\n",
    "print('Pattern shapes:', A_pattern.shape, B_pattern.shape, C_pattern.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the base patterns\n",
    "show_pattern(A_pattern, \"Letter A (base pattern)\")\n",
    "show_pattern(B_pattern, \"Letter B (base pattern)\")\n",
    "show_pattern(C_pattern, \"Letter C (base pattern)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5af022",
   "metadata": {},
   "source": [
    "## 2. Build the Dataset\n",
    "We will:\n",
    "- Use the base patterns for A, B, and C\n",
    "- Create a few **noisy variants** by flipping some pixels randomly\n",
    "- Stack everything into `X` (features) and `y` (labels)\n",
    "\n",
    "Labels:\n",
    "- A → 0\n",
    "- B → 1\n",
    "- C → 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17052c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(pattern, noise_level=0.1):\n",
    "    \"\"\"Randomly flips a fraction (noise_level) of pixels in the pattern.\"\"\"\n",
    "    noisy = pattern.copy()\n",
    "    num_pixels = len(pattern)\n",
    "    num_noisy = int(noise_level * num_pixels)\n",
    "    indices = np.random.choice(num_pixels, size=num_noisy, replace=False)\n",
    "    for idx in indices:\n",
    "        noisy[idx] = 1.0 - noisy[idx]  # flip 0 -> 1 or 1 -> 0\n",
    "    return noisy\n",
    "\n",
    "# Create dataset\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "num_variants = 10  # how many noisy copies per letter\n",
    "\n",
    "for _ in range(num_variants):\n",
    "    X_list.append(add_noise(A_pattern, noise_level=0.1))\n",
    "    y_list.append(0)\n",
    "    X_list.append(add_noise(B_pattern, noise_level=0.1))\n",
    "    y_list.append(1)\n",
    "    X_list.append(add_noise(C_pattern, noise_level=0.1))\n",
    "    y_list.append(2)\n",
    "\n",
    "# Also include the clean base patterns\n",
    "X_list.extend([A_pattern, B_pattern, C_pattern])\n",
    "y_list.extend([0, 1, 2])\n",
    "\n",
    "X = np.array(X_list)  # shape: (N, 30)\n",
    "y = np.array(y_list)  # shape: (N,)\n",
    "\n",
    "print('Dataset shape:', X.shape)\n",
    "print('Labels shape:', y.shape)\n",
    "print('Unique labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few random samples from the dataset\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(0, len(X))\n",
    "    label = y[idx]\n",
    "    letter = ['A', 'B', 'C'][label]\n",
    "    show_pattern(X[idx], f\"Sample {idx} (Label: {letter})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a681e",
   "metadata": {},
   "source": [
    "### One-Hot Encoding and Train/Test Split\n",
    "We convert labels to one-hot vectors (for softmax cross-entropy) and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes=3):\n",
    "    \"\"\"Convert label vector y into one-hot encoded matrix.\"\"\"\n",
    "    N = y.shape[0]\n",
    "    Y = np.zeros((N, num_classes))\n",
    "    Y[np.arange(N), y] = 1.0\n",
    "    return Y\n",
    "\n",
    "Y = one_hot_encode(y, num_classes=3)\n",
    "print('One-hot encoded Y shape:', Y.shape)\n",
    "\n",
    "# Simple train/test split (80% train, 20% test)\n",
    "num_samples = X.shape[0]\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(0.8 * num_samples)\n",
    "train_idx = indices[:split]\n",
    "test_idx = indices[split:]\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "print('Train set:', X_train.shape, Y_train.shape)\n",
    "print('Test set:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dba064",
   "metadata": {},
   "source": [
    "## 3. Define the Neural Network Architecture\n",
    "We build a **two-layer** neural network:\n",
    "- Input layer: 30 neurons (pixels)\n",
    "- Hidden layer: `hidden_size` neurons with **sigmoid** activation\n",
    "- Output layer: 3 neurons (A, B, C) with **softmax** activation\n",
    "\n",
    "We will implement:\n",
    "- Weight initialization\n",
    "- Forward pass\n",
    "- Loss function (cross-entropy)\n",
    "- Backpropagation and gradient descent update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744dd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and helpers\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    # derivative of sigmoid wrt its output a\n",
    "    return a * (1 - a)\n",
    "\n",
    "def softmax(z):\n",
    "    # subtract max for numerical stability\n",
    "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are (N, num_classes)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    y_pred_clipped = np.clip(y_pred, eps, 1 - eps)\n",
    "    loss = -np.sum(y_true * np.log(y_pred_clipped)) / y_true.shape[0]\n",
    "    return loss\n",
    "\n",
    "def accuracy(y_true_labels, y_pred_probs):\n",
    "    y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "    return np.mean(y_true_labels == y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1324eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters\n",
    "input_size = 30\n",
    "hidden_size = 16   # you can experiment with this\n",
    "output_size = 3\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(hidden_size, input_size) * 0.1\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size) * 0.1\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "W1.shape, b1.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebfbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    # X: (N, input_size)\n",
    "    Z1 = X @ W1.T + b1.T          # (N, hidden_size)\n",
    "    A1 = sigmoid(Z1)             # (N, hidden_size)\n",
    "    Z2 = A1 @ W2.T + b2.T        # (N, output_size)\n",
    "    A2 = softmax(Z2)             # (N, output_size)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64687509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation and parameter update\n",
    "def backward_pass(X, Y, Z1, A1, Z2, A2, W1, W2, learning_rate=0.1):\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Output layer gradient (softmax + cross-entropy)\n",
    "    dZ2 = A2 - Y                  # (N, output_size)\n",
    "    dW2 = (dZ2.T @ A1) / N        # (output_size, hidden_size)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True).T / N  # (output_size, 1)\n",
    "\n",
    "    # Hidden layer gradients\n",
    "    dA1 = dZ2 @ W2                # (N, hidden_size)\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)  # (N, hidden_size)\n",
    "    dW1 = (dZ1.T @ X) / N         # (hidden_size, input_size)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True).T / N  # (hidden_size, 1)\n",
    "\n",
    "    # Gradient descent update\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4474c1a",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "We train the network using **batch gradient descent**:\n",
    "- Forward pass on all training data\n",
    "- Compute loss and accuracy\n",
    "- Backpropagate errors and update weights\n",
    "- Repeat for many epochs\n",
    "\n",
    "We will store loss and accuracy per epoch and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 500\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Forward pass on training data\n",
    "    Z1_train, A1_train, Z2_train, A2_train = forward_pass(X_train, W1, b1, W2, b2)\n",
    "    loss_train = cross_entropy_loss(Y_train, A2_train)\n",
    "    acc_train = accuracy(y_train, A2_train)\n",
    "\n",
    "    # Backpropagation\n",
    "    W1, b1, W2, b2 = backward_pass(X_train, Y_train, Z1_train, A1_train, Z2_train, A2_train,\n",
    "                                   W1, W2, learning_rate=learning_rate)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    Z1_test, A1_test, Z2_test, A2_test = forward_pass(X_test, W1, b1, W2, b2)\n",
    "    loss_test = cross_entropy_loss(Y_test, A2_test)\n",
    "    acc_test = accuracy(y_test, A2_test)\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(loss_train)\n",
    "    train_accuracies.append(acc_train)\n",
    "    test_losses.append(loss_test)\n",
    "    test_accuracies.append(acc_test)\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d}: Train Loss={loss_train:.4f}, Train Acc={acc_train:.3f}, \"\n",
    "              f\"Test Loss={loss_test:.4f}, Test Acc={acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f6695",
   "metadata": {},
   "source": [
    "## 5. Visualize Loss and Accuracy\n",
    "We plot training and testing loss and accuracy over epochs to see how the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = np.arange(1, epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "plt.plot(epochs_range, test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(epochs_range, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53cd01c",
   "metadata": {},
   "source": [
    "## 6. Testing the Model and Visualizing Predictions\n",
    "We use the trained model to predict the labels for some test samples and display their images using `imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51559762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_pass(X, W1, b1, W2, b2)\n",
    "    return np.argmax(A2, axis=1), A2\n",
    "\n",
    "y_pred_test, y_pred_probs_test = predict(X_test, W1, b1, W2, b2)\n",
    "test_acc_final = np.mean(y_pred_test == y_test)\n",
    "print('Final Test Accuracy:', test_acc_final)\n",
    "\n",
    "# Show a few predictions\n",
    "for i in range(min(5, len(X_test))):\n",
    "    pattern = X_test[i]\n",
    "    true_label = y_test[i]\n",
    "    pred_label = y_pred_test[i]\n",
    "    true_letter = ['A', 'B', 'C'][true_label]\n",
    "    pred_letter = ['A', 'B', 'C'][pred_label]\n",
    "    show_pattern(pattern, f\"True: {true_letter}, Predicted: {pred_letter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b81e9",
   "metadata": {},
   "source": [
    "### Test on Clean Base Patterns\n",
    "Finally, we test the network on the original clean patterns for A, B, and C to see if it classifies them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fcf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = np.stack([A_pattern, B_pattern, C_pattern])\n",
    "y_clean = np.array([0, 1, 2])\n",
    "y_pred_clean, _ = predict(X_clean, W1, b1, W2, b2)\n",
    "\n",
    "for i in range(3):\n",
    "    true_letter = ['A', 'B', 'C'][y_clean[i]]\n",
    "    pred_letter = ['A', 'B', 'C'][y_pred_clean[i]]\n",
    "    show_pattern(X_clean[i], f\"Clean Pattern - True: {true_letter}, Predicted: {pred_letter}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
